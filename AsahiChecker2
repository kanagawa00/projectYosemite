from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support.ui import Select
import time
import pandas as pd
waseda_username="######"
keypass="#######"
#search_keyword="もう一つの祖国　戦後７０年"
keywordlist=["正月をたどって　戦後７０年","鏡の中の日本　戦後７０年・第１部","戦争のリアル　戦後７０年・第２部","国家と歴史　戦後７０年・第３部","沖縄　戦後７０年・第４部","広島・長崎・核　戦後７０年・第５部","平和のすがた　戦後７０年・第６部","国連と日本　戦後７０年・第７部","銅像をたどって　戦後７０年","バイブルをたどって　戦後７０年","戦後７０年　戦世を生きて","戦後７０年　中国","もう一つの祖国　戦後７０年","南方からの視線　戦後７０年","戦後７０年　日本の転換点","さまよう遺骨　戦後７０年","この人をたどって　戦後７０年","円が映す戦後７０年","戦後７０年　エピローグ","７０年目の首相","ナショナルと戦後の風景","核といのちを考える","戦争孤児の７０年","継ぐ記憶　私たちに戦争を教えてください","くらし７０年","もう一つの戦後　日系米国人の７０年"]
articles=[]

#一覧表示&テキストスクレイピングのプログラム
def newsdownload(browser,nl):
    browser.find_element_by_xpath('//input[@name="btnCheckOn"]').click()
    browser.find_element_by_xpath('//input[@name="btnDetail"]').click()
    numlist=browser.find_elements_by_xpath('//td[@class="topic-list"]//nobr')
    ttllist=browser.find_elements_by_class_name("font002")
    newslist=browser.find_elements_by_class_name("detail001")
    
    datelist=[]
    for i in range(int(len(numlist)/2)):
        datelist.append(str(numlist[i*2].text+numlist[i*2+1].text))
    for date,ttl,news in zip(datelist,ttllist,newslist):
        nl.append([date,ttl.text])
        print(date)
        print(ttl.text)
        print(news.text)
        print("■■■■■■■■■■■■■■■■■■")
    time.sleep(2)
    browser.find_element_by_xpath('//img[@alt="検索一覧画面へ戻る"]').click()
    time.sleep(1)
    try:
        browser.find_element_by_name("next").click()
    except:
        pass


def getpage(browser,search_keyword,newsdata):
#早稲田検索ページを開ける
    browser.get("http://waseda.summon.serialssolutions.com/jp/search?q=%E6%9C%9D%E6%97%A5%E6%96%B0%E8%81%9E&l=jp#!/search?ho=t&l=jp&q=%E6%9C%9D%E6%97%A5%E6%96%B0%E8%81%9E")
    time.sleep(3)
    targetlink=browser.find_element_by_xpath('//*[@ng-bind="::item.bet.title"]')
    targetlink.click()
    #新しいページでログインする
    allHandles = browser.window_handles
    for handle in allHandles:
        if browser.title.find("EZproxy") == -1:
            browser.switch_to_window(handle)
    #elem_user = browser.find_element_by_name("user")
    #elem_user.send_keys(waseda_username)
    #elem_pwd = browser.find_element_by_name("pass")
    #elem_pwd.send_keys(keypass)
    #browser.find_element_by_xpath('//*[@type="submit"]').click()
    #time.sleep(2)
    browser.find_element_by_xpath('//*[@alt="ログイン（Login）へ"]').click()
    time.sleep(3)
    browser.switch_to_frame("Introduce")
    browser.find_element_by_xpath('//input[@id="chkShishi2"]').click()
    browser.find_element_by_xpath('//input[@id="chkShishi3"]').click()
    browser.find_element_by_xpath('//input[@id="chkShishi4"]').click()
    #検索キーワードを入力する
    elem_inkw = browser.find_element_by_name("txtWord")
    elem_inkw.send_keys(search_keyword)
    
    browser.find_element_by_xpath('//input[@id="rdoSrchMode2"]').click()
    browser.find_element_by_xpath('//input[@id="rdoSrchItem3"]').click() #見出し
    browser.find_element_by_xpath('//input[@id="chkHochi2"]').click()
    browser.find_element_by_xpath('//input[@id="chkIssueS2"]').click()
    browser.find_element_by_xpath('//input[@id="chkIssueS3"]').click()
    browser.find_element_by_xpath('//input[@id="chkIssueS4"]').click()
    browser.find_element_by_xpath('//input[@id="chkIssueS5"]').click()

    Select(browser.find_element_by_name("cmbIDFy")).select_by_value("2015") 
    Select(browser.find_element_by_name("cmbIDFm")).select_by_value("01") 
    Select(browser.find_element_by_name("cmbIDFd")).select_by_value("01")
    Select(browser.find_element_by_name("cmbIDTy")).select_by_value("2015") 
    Select(browser.find_element_by_name("cmbIDTm")).select_by_value("12") 
    Select(browser.find_element_by_name("cmbIDTd")).select_by_value("31")

    time.sleep(1)
    browser.find_element_by_xpath('//input[@value="検索実行"]').click()

    #ページの切り替え
    w_start=1
    w_news=browser.find_element_by_xpath('//*[@class="fontcolor001"]')
    w_page=int(w_news.text)//20+1
    while w_start<=w_page:
        newsdownload(browser,newsdata)
        w_start+=1

    #browser.close()

browser = webdriver.Chrome()
for keyword in keywordlist:
    getpage(browser,keyword,articles)

browser.quit()
    
#データフレーム構築
df = pd.DataFrame(articles,columns=["時間","タイトル"])
df.to_csv("AsahiCheck2.csv",encoding="utf-8",sep="\t", index=False)
print(df)
